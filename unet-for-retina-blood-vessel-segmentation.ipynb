{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1604025,"sourceType":"datasetVersion","datasetId":946814}],"dockerImageVersionId":30512,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\nimport imageio\nfrom albumentations import HorizontalFlip,VerticalFlip,Rotate","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:07.823610Z","iopub.execute_input":"2023-07-16T02:33:07.823887Z","iopub.status.idle":"2023-07-16T02:33:10.360917Z","shell.execute_reply.started":"2023-07-16T02:33:07.823863Z","shell.execute_reply":"2023-07-16T02:33:10.355688Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\"\"\" Create a directory \"\"\"\n\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:10.366729Z","iopub.execute_input":"2023-07-16T02:33:10.369510Z","iopub.status.idle":"2023-07-16T02:33:10.381375Z","shell.execute_reply.started":"2023-07-16T02:33:10.369473Z","shell.execute_reply":"2023-07-16T02:33:10.376228Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def load_data(path):\n    \n    train_x = sorted(glob(os.path.join(path,\"training\",\"images\",\"*tif\")))\n    train_y = sorted(glob(os.path.join(path,\"training\",\"1st_manual\",\"*gif\")))\n    \n    \n    test_x = sorted(glob(os.path.join(path,\"test\",\"images\",\"*tif\")))\n    test_y = sorted(glob(os.path.join(path,\"test\",\"1st_manual\",\"*gif\")))\n    \n    return (train_x,train_y),(test_x,test_y)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:10.386147Z","iopub.execute_input":"2023-07-16T02:33:10.388555Z","iopub.status.idle":"2023-07-16T02:33:10.402539Z","shell.execute_reply.started":"2023-07-16T02:33:10.388520Z","shell.execute_reply":"2023-07-16T02:33:10.401497Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def augment_data(images, masks, save_path, augment=True):\n    size = (512, 512)\n\n    for idx, (x, y) in tqdm(enumerate (zip(images, masks)), total=len(images)):\n        \"\"\" Extracting the name \"\"\"\n        name = x.split(\"/\")[-1].split(\".\")[0]\n        \n        \"\"\" Reading image and mask \"\"\"\n        \n        x = cv2.imread(x,cv2.IMREAD_COLOR)\n        y = imageio.mimread(y)[0]\n        \n        if augment == True:\n            \n            aug = HorizontalFlip(p=1.0)\n            augmented = aug(image=x,mask=y)\n            \n            x1 = augmented[\"image\"]\n            y1 = augmented[\"mask\"]\n            \n            aug = VerticalFlip(p=1.0)\n            augmented = aug(image=x,mask=y)\n            x2 = augmented[\"image\"]\n            y2 = augmented[\"mask\"]\n            \n            aug = Rotate(limit=45,p=1.0)\n            augmented = aug(image=x,mask=y)\n            x3 = augmented[\"image\"]\n            y3= augmented[\"mask\"]\n            \n            X = [x, x1, x2, x3]\n            Y = [y, y1, y2, y3]\n        \n        else :\n            X = [x]\n            Y = [y]\n            \n            \n        index = 0    \n        for i,m in zip(X,Y):\n            \n            i = cv2.resize(i,size)\n            m = cv2.resize(m,size)\n            \n            tmp_image_name = f\"{name}_{index}.png\"\n            tmp_mask_name = f\"{name}_{index}.png\"\n\n            image_path = os.path.join(save_path,\"image\",tmp_image_name)\n            mask_path = os.path.join(save_path,\"mask\",tmp_mask_name)\n            \n            cv2.imwrite(image_path,i)\n            cv2.imwrite(mask_path,m)\n            \n            index += 1\n                   ","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:10.411359Z","iopub.execute_input":"2023-07-16T02:33:10.413240Z","iopub.status.idle":"2023-07-16T02:33:10.429354Z","shell.execute_reply.started":"2023-07-16T02:33:10.413200Z","shell.execute_reply":"2023-07-16T02:33:10.427611Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    \n    \"\"\" Seeding \"\"\"\n    np.random.seed(42)\n    \n    \"\"\" Load the data \"\"\"\n    data_path= \"/kaggle/input/drive2004/DRIVE\"\n    (train_x,train_y),(test_x,test_y) = load_data(data_path)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:10.431005Z","iopub.execute_input":"2023-07-16T02:33:10.431316Z","iopub.status.idle":"2023-07-16T02:33:10.594563Z","shell.execute_reply.started":"2023-07-16T02:33:10.431286Z","shell.execute_reply":"2023-07-16T02:33:10.593482Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(f\"Train: {len(train_x)} - {len(train_y)}\")\nprint(f\"Test: {len(test_x)} - {len(test_y)}\")\n\n\n\n\"\"\" Create directories to save the augmented data \"\"\"\n\ncreate_dir(\"new_data/train/image/\")\ncreate_dir(\"new_data/train/mask/\")\ncreate_dir(\"new_data/test/image/\")\ncreate_dir(\"new_data/test/mask/\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:10.597037Z","iopub.execute_input":"2023-07-16T02:33:10.599231Z","iopub.status.idle":"2023-07-16T02:33:10.610767Z","shell.execute_reply.started":"2023-07-16T02:33:10.599195Z","shell.execute_reply":"2023-07-16T02:33:10.609850Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Train: 20 - 20\nTest: 20 - 20\n","output_type":"stream"}]},{"cell_type":"code","source":"augment_data(train_x,train_y,\"new_data/train/\",augment=True)\naugment_data(test_x,test_y,\"new_data/test/\",augment=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:10.613331Z","iopub.execute_input":"2023-07-16T02:33:10.615217Z","iopub.status.idle":"2023-07-16T02:33:16.188352Z","shell.execute_reply.started":"2023-07-16T02:33:10.615150Z","shell.execute_reply":"2023-07-16T02:33:16.187422Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 20/20 [00:03<00:00,  5.51it/s]\n100%|██████████| 20/20 [00:01<00:00, 10.39it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:16.189792Z","iopub.execute_input":"2023-07-16T02:33:16.190784Z","iopub.status.idle":"2023-07-16T02:33:19.159370Z","shell.execute_reply.started":"2023-07-16T02:33:16.190749Z","shell.execute_reply":"2023-07-16T02:33:19.158413Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass conv_block(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_c)\n        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_c)\n        self.relu = nn.ReLU()\n        \n    def forward(self, inputs):\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        \n        return x\n    \n\nclass encoder_block(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        \n        self.conv = conv_block(in_c, out_c)\n        self.pool = nn.MaxPool2d((2, 2))\n        \n    def forward(self, inputs):\n        x = self.conv(inputs)\n        p = self.pool(x)\n        \n        return x, p\n    \n\nclass decoder_block(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        \n        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n        self.conv = conv_block(out_c + out_c, out_c)\n        \n    def forward(self, inputs, skip):\n        x = self.up(inputs)\n        x = torch.cat([x, skip], axis=1)\n        x = self.conv(x)\n        \n        return x\n    \n\nclass build_unet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        \"\"\" Encoder \"\"\"\n        \n        self.e1 = encoder_block(3, 64)\n        self.e2 = encoder_block(64, 128)\n        self.e3 = encoder_block(128, 256)\n        self.e4 = encoder_block(256, 512)\n        \n        \"\"\" Bottleneck \"\"\"\n        \n        self.b = conv_block(512, 1024)\n        \n        \"\"\" Decoder \"\"\"\n        \n        self.d1 = decoder_block(1024, 512)\n        self.d2 = decoder_block(512, 256)\n        self.d3 = decoder_block(256, 128)\n        self.d4 = decoder_block(128, 64)\n        \n        \"\"\" Classifier \"\"\"\n        self.outputs = nn.Conv2d(64,1,kernel_size=1,padding=0)\n        \n    def forward(self, inputs):\n        \"\"\"Encoder\"\"\"\n        s1, p1 = self.e1(inputs)\n        s2, p2 = self.e2(p1)\n        s3, p3 = self.e3(p2)\n        s4, p4 = self.e4(p3)\n        \n        \"\"\" Bottleneck \"\"\"\n        \n        b = self.b(p4)\n        \n        \n        \n        \"\"\" Decoder \"\"\"\n        \n        d1 = self.d1(b, s4)\n        d2 = self.d2(d1, s3)\n        d3 = self.d3(d2, s2)\n        d4 = self.d4(d3, s1)\n        \n        outputs = self.outputs(d4)\n        \n        return outputs  \n        \n\nif __name__ == \"__main__\":\n    x = torch.randn((2, 3, 512, 512))\n    f = build_unet()\n    y = f(x)\n    print(y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:19.160943Z","iopub.execute_input":"2023-07-16T02:33:19.161861Z","iopub.status.idle":"2023-07-16T02:33:33.254652Z","shell.execute_reply.started":"2023-07-16T02:33:19.161824Z","shell.execute_reply":"2023-07-16T02:33:33.253421Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([2, 1, 512, 512])\n","output_type":"stream"}]},{"cell_type":"raw","source":"import os\nimport time\nfrom glob import glob\n\n\nimport torch \nfrom torch.utils.data import DataLoader\nimport torch.nn as nn","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport torch\n\n\ndef train(model,loader,optimizer,loss_fn,device):\n    epoch_loss = 0.0 \n    model.train()\n    for x,y in loader:\n        x = x.to(device,dtype=torch.float32)\n        y = y.to(device,dtype=torch.float32)\n        \n        optimizer.zero_grad()\n        y_pred = model(x)\n        loss = loss_fn(y_pred,y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    epoch_loss = epoch_loss/len(loader)\n    return epoch_loss\n\n\ndef evaluate(model,loader,loss_fn,device):\n    \n    epoch_loss = 0.0\n    model.eval()\n    with torch.no_grad():\n        for x,y in loader:\n            x = x.to(device,dtype=torch.float32)\n            y = y.to(device,dtype=torch.float32)\n\n            \n            y_pred = model(x)\n            loss = loss_fn(y_pred,y)\n            epoch_loss += loss.item()\n        \n        epoch_loss = epoch_loss/len(loader)\n        \n    return epoch_loss\n            \n            \ndef seeding(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nif __name__ == \"__main__\":\n    \"\"\" Seeding \"\"\"\n    seeding(42)\n\n    \"\"\" Directories \"\"\"\n    create_dir(\"files\")\n\n    \"\"\" Load dataset\"\"\"\n\n    train_x = sorted(glob(\"/kaggle/working/new_data/train/image/*\"))[:20]\n    train_y = sorted(glob(\"/kaggle/working/new_data/train/mask/*\"))[:20]\n\n    valid_x = sorted(glob(\"/kaggle/working/new_data/test/image/*\"))\n    valid_y = sorted(glob(\"/kaggle/working/new_data/test/mask/*\")) \n\n    data_str = f\"Dataset Size:\\nTrain:{len(train_x)}-Valid:{len(valid_x)}\\n\"\n    print(data_str)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T03:21:15.950330Z","iopub.execute_input":"2023-07-16T03:21:15.950740Z","iopub.status.idle":"2023-07-16T03:21:15.967431Z","shell.execute_reply.started":"2023-07-16T03:21:15.950709Z","shell.execute_reply":"2023-07-16T03:21:15.966285Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Dataset Size:\nTrain:20-Valid:20\n\n","output_type":"stream"}]},{"cell_type":"code","source":"H = 512\nW = 512\n\nsize = (H,W)\nbatch_size = 2\nnum_epochs = 50\nlr = 1e-4\ncheckpoint_path = \"files/checkpoint.pth\"","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:33:33.274748Z","iopub.execute_input":"2023-07-16T02:33:33.275574Z","iopub.status.idle":"2023-07-16T02:33:33.280328Z","shell.execute_reply.started":"2023-07-16T02:33:33.275548Z","shell.execute_reply":"2023-07-16T02:33:33.279340Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\"\"\" Dataset and the loader \"\"\"\nfrom torch.utils.data import Dataset\n\n\nclass DriveDataset(Dataset):\n    \n    def __init__(self, images_path, masks_path):\n        self.images_path = images_path\n        self.masks_path = masks_path\n        self.n_samples = len(images_path)\n        \n    def __getitem__(self, index):\n        \"\"\" Reading image \"\"\"\n        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n        image = image / 255.0 \n        image = np.transpose(image, (2, 0, 1))\n        image = image.astype(np.float32)\n        image = torch.from_numpy(image)\n        \n        \"\"\" Reading masks \"\"\"\n        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n        mask = mask / 255.0 \n        mask = np.expand_dims(mask, axis=0)\n        mask = mask.astype(np.float32)\n        mask = torch.from_numpy(mask)\n        \n        return image, mask\n    \n    def __len__(self):\n        return self.n_samples\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:59:27.799640Z","iopub.execute_input":"2023-07-16T02:59:27.800717Z","iopub.status.idle":"2023-07-16T02:59:27.816732Z","shell.execute_reply.started":"2023-07-16T02:59:27.800669Z","shell.execute_reply":"2023-07-16T02:59:27.815729Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_dataset = DriveDataset(train_x,train_y)\nvalid_dataset = DriveDataset(valid_x,valid_y)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:59:30.222604Z","iopub.execute_input":"2023-07-16T02:59:30.222959Z","iopub.status.idle":"2023-07-16T02:59:30.227578Z","shell.execute_reply.started":"2023-07-16T02:59:30.222929Z","shell.execute_reply":"2023-07-16T02:59:30.226498Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"\"\"\" Loader \"\"\"\nfrom torch.utils.data import DataLoader\ntrain_loader = DataLoader(\n    \n    dataset = train_dataset,\n    batch_size = batch_size,\n    shuffle = True,\n    num_workers = 2\n)\n\nvalid_loader = DataLoader(\n    \n    dataset = valid_dataset,\n    batch_size = batch_size,\n    shuffle = False,\n    num_workers = 2\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:59:30.549711Z","iopub.execute_input":"2023-07-16T02:59:30.550108Z","iopub.status.idle":"2023-07-16T02:59:30.557779Z","shell.execute_reply.started":"2023-07-16T02:59:30.550076Z","shell.execute_reply":"2023-07-16T02:59:30.556749Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda')\nmodel = build_unet()\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:59:31.020708Z","iopub.execute_input":"2023-07-16T02:59:31.021089Z","iopub.status.idle":"2023-07-16T02:59:31.343031Z","shell.execute_reply.started":"2023-07-16T02:59:31.021058Z","shell.execute_reply":"2023-07-16T02:59:31.342034Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n\n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = torch.sigmoid(inputs)\n\n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n\n        intersection = (inputs * targets).sum()\n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n\n        return 1 - dice\n\nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n\n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = torch.sigmoid(inputs)\n\n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n\n        intersection = (inputs * targets).sum()\n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n\n        return Dice_BCE","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:59:31.420794Z","iopub.execute_input":"2023-07-16T02:59:31.421136Z","iopub.status.idle":"2023-07-16T02:59:31.432083Z","shell.execute_reply.started":"2023-07-16T02:59:31.421107Z","shell.execute_reply":"2023-07-16T02:59:31.430905Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(),lr=lr)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',patience=5,verbose=True)\nloss_fn = DiceBCELoss()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T02:59:31.992867Z","iopub.execute_input":"2023-07-16T02:59:31.993794Z","iopub.status.idle":"2023-07-16T02:59:32.000375Z","shell.execute_reply.started":"2023-07-16T02:59:31.993745Z","shell.execute_reply":"2023-07-16T02:59:31.999396Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"execution":{"iopub.status.busy":"2023-07-16T03:14:47.457685Z","iopub.execute_input":"2023-07-16T03:14:47.458116Z","iopub.status.idle":"2023-07-16T03:14:47.464376Z","shell.execute_reply.started":"2023-07-16T03:14:47.458069Z","shell.execute_reply":"2023-07-16T03:14:47.463439Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"\"\"\" Training the model \"\"\"\nimport time\n\nbest_valid_loss = float(\"inf\")\n\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    \n    \n    train_loss = train(model,train_loader,optimizer,loss_fn,device)\n    valid_loss = evaluate(model,valid_loader,loss_fn,device)\n    \n    \"\"\" Saving the model \"\"\"\n    \n    if valid_loss<best_valid_loss:\n        data_str = f\"Valid loss imporved from {best_valid_loss:2.4f} to {valid_loss}\"\n        print(data_str)\n    \n    end_time = time.time()\n    \n    epoch_mins,epoch_secs = epoch_time(start_time,end_time)\n    \n    data_str = f'Epoch : {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s'\n    data_str += f'\\tTrain Loss: {train_loss: .3f}\\n'\n    data_str += f'\\t Val.Loss : {valid_loss: .3f}\\n'\n    print(data_str)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-16T03:25:45.340159Z","iopub.execute_input":"2023-07-16T03:25:45.341098Z","iopub.status.idle":"2023-07-16T03:28:49.829568Z","shell.execute_reply.started":"2023-07-16T03:25:45.341036Z","shell.execute_reply":"2023-07-16T03:28:49.827441Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Valid loss imporved from inf to 0.49821129739284514\nEpoch : 01 | Epoch Time: 0m 3s\tTrain Loss:  0.290\n\t Val.Loss :  0.498\n\nValid loss imporved from inf to 0.5099482417106629\nEpoch : 02 | Epoch Time: 0m 3s\tTrain Loss:  0.289\n\t Val.Loss :  0.510\n\nValid loss imporved from inf to 0.4942701131105423\nEpoch : 03 | Epoch Time: 0m 3s\tTrain Loss:  0.286\n\t Val.Loss :  0.494\n\nValid loss imporved from inf to 0.49403526484966276\nEpoch : 04 | Epoch Time: 0m 3s\tTrain Loss:  0.285\n\t Val.Loss :  0.494\n\nValid loss imporved from inf to 0.4915975838899612\nEpoch : 05 | Epoch Time: 0m 4s\tTrain Loss:  0.278\n\t Val.Loss :  0.492\n\nValid loss imporved from inf to 0.48781625032424925\nEpoch : 06 | Epoch Time: 0m 3s\tTrain Loss:  0.275\n\t Val.Loss :  0.488\n\nValid loss imporved from inf to 0.48522281348705293\nEpoch : 07 | Epoch Time: 0m 3s\tTrain Loss:  0.273\n\t Val.Loss :  0.485\n\nValid loss imporved from inf to 0.48113332986831664\nEpoch : 08 | Epoch Time: 0m 3s\tTrain Loss:  0.270\n\t Val.Loss :  0.481\n\nValid loss imporved from inf to 0.4852107763290405\nEpoch : 09 | Epoch Time: 0m 3s\tTrain Loss:  0.267\n\t Val.Loss :  0.485\n\nValid loss imporved from inf to 0.47960625886917113\nEpoch : 10 | Epoch Time: 0m 3s\tTrain Loss:  0.264\n\t Val.Loss :  0.480\n\nValid loss imporved from inf to 0.4843073904514313\nEpoch : 11 | Epoch Time: 0m 3s\tTrain Loss:  0.261\n\t Val.Loss :  0.484\n\nValid loss imporved from inf to 0.4782803326845169\nEpoch : 12 | Epoch Time: 0m 3s\tTrain Loss:  0.260\n\t Val.Loss :  0.478\n\nValid loss imporved from inf to 0.48212957978248594\nEpoch : 13 | Epoch Time: 0m 3s\tTrain Loss:  0.257\n\t Val.Loss :  0.482\n\nValid loss imporved from inf to 0.4780089259147644\nEpoch : 14 | Epoch Time: 0m 3s\tTrain Loss:  0.255\n\t Val.Loss :  0.478\n\nValid loss imporved from inf to 0.4792759656906128\nEpoch : 15 | Epoch Time: 0m 3s\tTrain Loss:  0.254\n\t Val.Loss :  0.479\n\nValid loss imporved from inf to 0.4793569654226303\nEpoch : 16 | Epoch Time: 0m 3s\tTrain Loss:  0.251\n\t Val.Loss :  0.479\n\nValid loss imporved from inf to 0.481621715426445\nEpoch : 17 | Epoch Time: 0m 3s\tTrain Loss:  0.249\n\t Val.Loss :  0.482\n\nValid loss imporved from inf to 0.4719423145055771\nEpoch : 18 | Epoch Time: 0m 3s\tTrain Loss:  0.248\n\t Val.Loss :  0.472\n\nValid loss imporved from inf to 0.4734251260757446\nEpoch : 19 | Epoch Time: 0m 3s\tTrain Loss:  0.247\n\t Val.Loss :  0.473\n\nValid loss imporved from inf to 0.46950654685497284\nEpoch : 20 | Epoch Time: 0m 3s\tTrain Loss:  0.245\n\t Val.Loss :  0.470\n\nValid loss imporved from inf to 0.4711298316717148\nEpoch : 21 | Epoch Time: 0m 3s\tTrain Loss:  0.245\n\t Val.Loss :  0.471\n\nValid loss imporved from inf to 0.4790039509534836\nEpoch : 22 | Epoch Time: 0m 3s\tTrain Loss:  0.245\n\t Val.Loss :  0.479\n\nValid loss imporved from inf to 0.4744679778814316\nEpoch : 23 | Epoch Time: 0m 3s\tTrain Loss:  0.243\n\t Val.Loss :  0.474\n\nValid loss imporved from inf to 0.47861488461494445\nEpoch : 24 | Epoch Time: 0m 3s\tTrain Loss:  0.241\n\t Val.Loss :  0.479\n\nValid loss imporved from inf to 0.4615728765726089\nEpoch : 25 | Epoch Time: 0m 3s\tTrain Loss:  0.238\n\t Val.Loss :  0.462\n\nValid loss imporved from inf to 0.46088455319404603\nEpoch : 26 | Epoch Time: 0m 3s\tTrain Loss:  0.237\n\t Val.Loss :  0.461\n\nValid loss imporved from inf to 0.4589651048183441\nEpoch : 27 | Epoch Time: 0m 3s\tTrain Loss:  0.235\n\t Val.Loss :  0.459\n\nValid loss imporved from inf to 0.4577589422464371\nEpoch : 28 | Epoch Time: 0m 3s\tTrain Loss:  0.233\n\t Val.Loss :  0.458\n\nValid loss imporved from inf to 0.461020240187645\nEpoch : 29 | Epoch Time: 0m 3s\tTrain Loss:  0.230\n\t Val.Loss :  0.461\n\nValid loss imporved from inf to 0.4574355512857437\nEpoch : 30 | Epoch Time: 0m 3s\tTrain Loss:  0.230\n\t Val.Loss :  0.457\n\nValid loss imporved from inf to 0.4589558571577072\nEpoch : 31 | Epoch Time: 0m 3s\tTrain Loss:  0.228\n\t Val.Loss :  0.459\n\nValid loss imporved from inf to 0.45728982985019684\nEpoch : 32 | Epoch Time: 0m 3s\tTrain Loss:  0.227\n\t Val.Loss :  0.457\n\nValid loss imporved from inf to 0.45793242156505587\nEpoch : 33 | Epoch Time: 0m 3s\tTrain Loss:  0.224\n\t Val.Loss :  0.458\n\nValid loss imporved from inf to 0.4571615129709244\nEpoch : 34 | Epoch Time: 0m 3s\tTrain Loss:  0.223\n\t Val.Loss :  0.457\n\nValid loss imporved from inf to 0.4553369671106339\nEpoch : 35 | Epoch Time: 0m 3s\tTrain Loss:  0.220\n\t Val.Loss :  0.455\n\nValid loss imporved from inf to 0.4528737097978592\nEpoch : 36 | Epoch Time: 0m 3s\tTrain Loss:  0.220\n\t Val.Loss :  0.453\n\nValid loss imporved from inf to 0.45665192008018496\nEpoch : 37 | Epoch Time: 0m 3s\tTrain Loss:  0.217\n\t Val.Loss :  0.457\n\nValid loss imporved from inf to 0.45030627250671384\nEpoch : 38 | Epoch Time: 0m 3s\tTrain Loss:  0.216\n\t Val.Loss :  0.450\n\nValid loss imporved from inf to 0.45454804301261903\nEpoch : 39 | Epoch Time: 0m 3s\tTrain Loss:  0.215\n\t Val.Loss :  0.455\n\nValid loss imporved from inf to 0.45154343247413636\nEpoch : 40 | Epoch Time: 0m 3s\tTrain Loss:  0.214\n\t Val.Loss :  0.452\n\nValid loss imporved from inf to 0.453445228934288\nEpoch : 41 | Epoch Time: 0m 3s\tTrain Loss:  0.214\n\t Val.Loss :  0.453\n\nValid loss imporved from inf to 0.44983674883842467\nEpoch : 42 | Epoch Time: 0m 3s\tTrain Loss:  0.211\n\t Val.Loss :  0.450\n\nValid loss imporved from inf to 0.4494206339120865\nEpoch : 43 | Epoch Time: 0m 3s\tTrain Loss:  0.211\n\t Val.Loss :  0.449\n\nValid loss imporved from inf to 0.448380121588707\nEpoch : 44 | Epoch Time: 0m 3s\tTrain Loss:  0.209\n\t Val.Loss :  0.448\n\nValid loss imporved from inf to 0.44968431293964384\nEpoch : 45 | Epoch Time: 0m 3s\tTrain Loss:  0.208\n\t Val.Loss :  0.450\n\nValid loss imporved from inf to 0.4539683014154434\nEpoch : 46 | Epoch Time: 0m 3s\tTrain Loss:  0.208\n\t Val.Loss :  0.454\n\nValid loss imporved from inf to 0.44849168956279756\nEpoch : 47 | Epoch Time: 0m 3s\tTrain Loss:  0.208\n\t Val.Loss :  0.448\n\nValid loss imporved from inf to 0.45371705293655396\nEpoch : 48 | Epoch Time: 0m 4s\tTrain Loss:  0.207\n\t Val.Loss :  0.454\n\nValid loss imporved from inf to 0.4466305077075958\nEpoch : 49 | Epoch Time: 0m 3s\tTrain Loss:  0.207\n\t Val.Loss :  0.447\n\nValid loss imporved from inf to 0.4461260408163071\nEpoch : 50 | Epoch Time: 0m 3s\tTrain Loss:  0.206\n\t Val.Loss :  0.446\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}